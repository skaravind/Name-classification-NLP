{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120719/120719 [00:06<00:00, 20119.27it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gensim \n",
    "from gensim.models import Word2Vec \n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import numpy as np\n",
    "  \n",
    "dataset = pd.read_csv(\"data/names_combined.csv\")\n",
    "\n",
    "dataset.dropna(inplace=True)\n",
    "dataset.replace('b','black',inplace=True)\n",
    "  \n",
    "data = [] \n",
    "corpus = []\n",
    "\n",
    "def features(name):\n",
    "    name = re.sub('[^a-zA-Z]', ' ', name)\n",
    "    name = re.sub('  ', ' ', name)\n",
    "    name = name.lower()\n",
    "    name = name[:30]\n",
    "    name += '#'*(30-len(name))\n",
    "    bigrams = []\n",
    "    trigrams = []\n",
    "    for i in range(len(name)-1):\n",
    "        bigrams.append(name[i:i+2])\n",
    "        if i < len(name) - 2:\n",
    "            trigrams.append(name[i:i+3])\n",
    "    trigrams.append(name[-3:])\n",
    "    return list(name)+bigrams#+trigrams\n",
    "\n",
    "for i in tqdm(range(len(dataset['name']))):\n",
    "    temp = []\n",
    "    review = dataset['name'].values[i]\n",
    "    review = re.sub('[^a-zA-Z]', ' ', review)\n",
    "    review = re.sub('  ', ' ', review)\n",
    "    if review[0] == ' ':\n",
    "        review=review[1:]\n",
    "    temp = features(review)\n",
    "    data.append(temp)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model created\n",
      "model trained\n"
     ]
    }
   ],
   "source": [
    "model1 = gensim.models.Word2Vec(data, min_count = 1, size = 30, window = 10) ## 300,10\n",
    "print('model created')\n",
    "model1.train(data,total_examples=model1.corpus_count, epochs=10)\n",
    "print('model trained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7080"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs = model1[features('rajes')]#.tolist()\n",
    "vecs.nbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/120719 [00:00<?, ?it/s]/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  after removing the cwd from sys.path.\n",
      "100%|██████████| 120719/120719 [01:36<00:00, 1250.29it/s]\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = dataset.iloc[:, 2].values\n",
    "for c in tqdm(corpus):\n",
    "    vecs = model1[features(c)].tolist()\n",
    "    X += [vecs]\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X, y = ros.fit_resample(X.reshape((120719, 59*30)), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape((194580,59*30))\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "y = enc.fit_transform(y.reshape(-1,1)).toarray()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)#, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=1770, activation=\"relu\", units=1024)`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=256)`\n",
      "  del sys.path[0]\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=128)`\n",
      "  app.launch_new_instance()\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:19: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=128)`\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=64)`\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:25: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=32)`\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:28: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"softmax\", units=4)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 155664 samples, validate on 38916 samples\n",
      "Epoch 1/40\n",
      "155664/155664 [==============================] - 71s 453us/step - loss: 0.7214 - acc: 0.6803 - val_loss: 0.5882 - val_acc: 0.7475\n",
      "Epoch 2/40\n",
      "155664/155664 [==============================] - 55s 351us/step - loss: 0.5307 - acc: 0.7718 - val_loss: 0.5353 - val_acc: 0.7771\n",
      "Epoch 3/40\n",
      "155664/155664 [==============================] - 55s 351us/step - loss: 0.4837 - acc: 0.7918 - val_loss: 0.4945 - val_acc: 0.7952\n",
      "Epoch 4/40\n",
      "155664/155664 [==============================] - 56s 358us/step - loss: 0.4572 - acc: 0.8032 - val_loss: 0.4954 - val_acc: 0.7915\n",
      "Epoch 5/40\n",
      "155664/155664 [==============================] - 53s 341us/step - loss: 0.4387 - acc: 0.8109 - val_loss: 0.4488 - val_acc: 0.8095\n",
      "Epoch 6/40\n",
      "155664/155664 [==============================] - 53s 341us/step - loss: 0.4239 - acc: 0.8174 - val_loss: 0.4780 - val_acc: 0.7994\n",
      "Epoch 7/40\n",
      "155648/155664 [============================>.] - ETA: 0s - loss: 0.4121 - acc: 0.8219"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, GRU, LSTM, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import optimizers\n",
    "\n",
    "classifier = Sequential()\n",
    "\n",
    "######## race classification 87% test accuracy\n",
    "classifier.add(Dense(input_dim=59*30, output_dim=1024, activation='relu'))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(Dropout(0.2))  # Dropout overfitting\n",
    "classifier.add(Dense(output_dim=256, activation='relu'))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(Dropout(0.1))\n",
    "classifier.add(Dense(output_dim=128, activation='relu'))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(Dropout(0.1))\n",
    "classifier.add(Dense(output_dim=128, activation='relu'))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(Dropout(0.1))\n",
    "classifier.add(Dense(output_dim=64, activation='relu'))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(Dropout(0.1))\n",
    "classifier.add(Dense(output_dim=32, activation='relu'))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(Dropout(0.1))\n",
    "classifier.add(Dense(output_dim=4, activation='softmax'))\n",
    "\n",
    "##### gender classification model 98% test acc\n",
    "# classifier1.add(Dense(input_dim=300, output_dim=256, activation='relu'))\n",
    "# classifier1.add(BatchNormalization())\n",
    "# classifier1.add(Dropout(0.1))  # Dropout overfitting\n",
    "# classifier1.add(Dense(output_dim=128, activation='relu'))\n",
    "# classifier1.add(BatchNormalization())\n",
    "# classifier1.add(Dropout(0.1))\n",
    "# classifier1.add(Dense(output_dim=128, activation='relu'))\n",
    "# classifier1.add(BatchNormalization())\n",
    "# classifier1.add(Dropout(0.1))\n",
    "# classifier1.add(Dense(output_dim=64, activation='relu'))\n",
    "# classifier1.add(BatchNormalization())\n",
    "# classifier1.add(Dropout(0.1))\n",
    "# classifier1.add(Dense(output_dim=64, activation='relu'))\n",
    "# classifier1.add(BatchNormalization())\n",
    "# classifier1.add(Dropout(0.1))\n",
    "# classifier1.add(Dense(output_dim=32, activation='relu'))\n",
    "# classifier1.add(BatchNormalization())\n",
    "# classifier1.add(Dropout(0.1))\n",
    "# classifier1.add(Dense(output_dim=2, activation='softmax'))\n",
    "\n",
    "\n",
    "classifier.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics = ['accuracy'])\n",
    "history = classifier.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=40, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channels_first\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:58: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, kernel_size=(3, 3), activation=\"relu\", padding=\"same\")`\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:64: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, kernel_size=(3, 3), activation=\"relu\", padding=\"same\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_11 (Conv2D)           (None, 64, 59, 30)        640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 64, 59, 30)        120       \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 64, 59, 30)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 64, 59, 30)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 64, 59, 30)        120       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 64, 29, 15)        0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 64, 29, 15)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 64, 29, 15)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 64, 29, 15)        60        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 64, 14, 7)         0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 64, 14, 7)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 500)               3136000   \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4)                 2004      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 3,214,800\n",
      "Trainable params: 3,213,650\n",
      "Non-trainable params: 1,150\n",
      "_________________________________________________________________\n",
      "Train on 155664 samples, validate on 38916 samples\n",
      "Epoch 1/30\n",
      "  2816/155664 [..............................] - ETA: 2:03:12 - loss: 1.5186 - acc: 0.4542"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-9fc022998331>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m           validation_data=(X_test, y_test),shuffle=True,callbacks=callbacks)\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation, BatchNormalization, regularizers\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.utils.np_utils import to_categorical\n",
    "K.set_image_dim_ordering('th')\n",
    "print(K.image_data_format())\n",
    "\n",
    "import os\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import numpy as np # linear algebra\n",
    "\n",
    "# define path to save model\n",
    "model_path = './fm_cnn_BN.h5'\n",
    "# prepare callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_acc', \n",
    "        patience=10,\n",
    "        mode='max',\n",
    "        verbose=1),\n",
    "    ModelCheckpoint(model_path,\n",
    "        monitor='val_acc', \n",
    "        save_best_only=True, \n",
    "        mode='max',\n",
    "        verbose=0)\n",
    "]\n",
    "\n",
    "\n",
    "#size of parameters\n",
    "batch_size = 256\n",
    "num_classes = 4\n",
    "epochs = 30\n",
    "filter_pixel=3\n",
    "noise = 1\n",
    "droprate=0.25\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 59, 30\n",
    "\n",
    "input_shape = (1, img_rows, img_cols)\n",
    "\n",
    "#Start Neural Network\n",
    "model = Sequential()\n",
    "#convolution 1st layer\n",
    "model.add(Conv2D(64, kernel_size=(filter_pixel, filter_pixel), padding=\"same\",\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape)) #0\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(droprate))#3\n",
    "#model.add(MaxPooling2D())\n",
    "\n",
    "#convolution 2nd layer\n",
    "model.add(Conv2D(64, kernel_size=(filter_pixel, filter_pixel), activation='relu',border_mode=\"same\"))#1\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Dropout(droprate))#3\n",
    "\n",
    "#convolution 3rd layer\n",
    "model.add(Conv2D(64, kernel_size=(filter_pixel, filter_pixel), activation='relu',border_mode=\"same\"))#1\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Dropout(droprate))#3\n",
    "\n",
    "#Fully connected 1st layer\n",
    "model.add(Flatten()) #7\n",
    "model.add(Dense(500,use_bias=False)) #13\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu')) #14\n",
    "model.add(Dropout(droprate))      #15\n",
    "\n",
    "#Fully connected final layer\n",
    "model.add(Dense(4)) #8\n",
    "model.add(Activation('softmax')) #9\n",
    "\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "#Save Model=ON\n",
    "history = model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, y_test),shuffle=True,callbacks=callbacks)\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "#print loss and accuracy\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
